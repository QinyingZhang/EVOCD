"""
æŸå¤±å‡½æ•°æ¶ˆèå®éªŒ2ï¼šåŠ æƒäº¤å‰ç†µï¼ˆWeighted CEï¼Œæ— è¿‡é‡‡æ ·ï¼Œæ— æ•°æ®å¢å¼ºï¼‰
ä½¿ç”¨ç®€å•æ‹¼æ¥èåˆæ¨¡å‹
è®­ç»ƒç­–ç•¥ï¼šå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰
"""
import argparse
import sys
from pathlib import Path
from typing import cast
import torch
import torch.nn as nn

# æ·»åŠ hier_multç›®å½•åˆ°Pythonè·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
HIER_MULT_DIR = PROJECT_ROOT / 'hier_mult'

# ç¡®ä¿hier_multç›®å½•åœ¨Pythonè·¯å¾„ä¸­
if str(HIER_MULT_DIR) not in sys.path:
    sys.path.insert(0, str(HIER_MULT_DIR))

# å¯¼å…¥hier_multæ¨¡å—
from archived.model_concat_fusion import HierarchicalFloodClassifier_ConcatFusion  # type: ignore
from settings import ORIENTATION_MODEL, COMPONENT_MODELS, NUM_FLOOD_GRADES  # type: ignore
from train import Trainer  # type: ignore


if __name__ == '__main__':
    print("=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ2ï¼šåŠ æƒäº¤å‰ç†µ")
    print("Weighted CE | ç±»åˆ«æƒé‡ | æ— è¿‡é‡‡æ · | æ— æ•°æ®å¢å¼º")
    print("=" * 70)
    
    # è·å–æ•°æ®é›†ç›®å½•ï¼ˆAll_ablation_experiments/dataï¼‰
    SCRIPT_DIR = Path(__file__).parent
    PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent
    DATA_DIR = PROJECT_ROOT / 'All_ablation_experiments' / 'data'
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='è®­ç»ƒç­–ç•¥æ¶ˆèï¼šåŠ æƒäº¤å‰ç†µ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_csv', type=str, default=str(DATA_DIR / 'train.csv'))
    parser.add_argument('--val_csv', type=str, default=str(DATA_DIR / 'val.csv'))
    parser.add_argument('--test_csv', type=str, default=str(DATA_DIR / 'test.csv'))
    parser.add_argument('--image_size', type=int, default=640)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=4)  # å‡å°æ‰¹é‡å¤§å°ä»¥èŠ‚çœæ˜¾å­˜
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--pin_memory', type=bool, default=True)
    
    # æ¨¡å‹å‚æ•° - å…³é”®ï¼šä¸å†»ç»“ä»»ä½•æ¨¡å—ï¼ˆå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼‰
    parser.add_argument('--freeze_backbone', type=bool, default=False)  # ä¸å†»ç»“éª¨å¹²ç½‘ç»œ
    parser.add_argument('--use_component_branch', type=bool, default=True)
    
    # æ•°æ®å¢å¼º - å…³é—­
    parser.add_argument('--augment', type=bool, default=False)
    parser.add_argument('--oversample', type=bool, default=False)
    parser.add_argument('--oversample_level0', type=int, default=1)
    parser.add_argument('--oversample_level1', type=int, default=1)
    
    # æŸå¤±å‡½æ•° - åŠ æƒäº¤å‰ç†µ
    parser.add_argument('--use_class_weights', type=bool, default=True)  # å¼€å¯ç±»åˆ«æƒé‡
    parser.add_argument('--label_smoothing', type=float, default=0.0)
    parser.add_argument('--use_focal_loss', type=bool, default=False)
    parser.add_argument('--focal_gamma', type=float, default=1.5)  # ç»Ÿä¸€ä¸º1.5ï¼ˆè™½ç„¶ä¸ä½¿ç”¨Focal Lossï¼‰
    
    # è®­ç»ƒç­–ç•¥
    parser.add_argument('--mixed_precision', type=bool, default=True)
    parser.add_argument('--patience', type=int, default=20)
    parser.add_argument('--min_delta', type=float, default=1e-4)
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument('--print_freq', type=int, default=10)
    parser.add_argument('--save_interval', type=int, default=10)
    parser.add_argument('--resume', type=str, default=None)
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--runs_dir', type=str, default=str(SCRIPT_DIR / 'runs' / 'strategy_2_weighted_ce'))
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä½¿ç”¨åŸå§‹çš„ Trainer ç±»ï¼‰
    trainer = Trainer(args)
    
    # æ›¿æ¢æ¨¡å‹ä¸ºæ¶ˆèå®éªŒç‰ˆæœ¬ï¼ˆç®€å•æ‹¼æ¥èåˆï¼‰
    print("\næ›¿æ¢ä¸ºæ¶ˆèå®éªŒæ¨¡å‹ï¼ˆç®€å•æ‹¼æ¥èåˆï¼‰...")
    component_paths = {k: str(v) for k, v in COMPONENT_MODELS.items()}
    
    trainer.model = HierarchicalFloodClassifier_ConcatFusion(
        orientation_model_path=str(ORIENTATION_MODEL),
        component_model_paths=component_paths,
        num_flood_classes=NUM_FLOOD_GRADES,
        freeze_backbone=False,  # ä¸å†»ç»“ï¼Œä½¿ç”¨å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒ
        use_component_branch=True  # ä½¿ç”¨ç»„ä»¶åˆ†æ”¯
    ).to(trainer.device)
    
    print("âœ… æ¨¡å‹æ›¿æ¢å®Œæˆ")
    print(f"èåˆæ–¹å¼: ç®€å•æ‹¼æ¥ï¼ˆConcatï¼‰")
    
    print("\n" + "=" * 70)
    print("å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆæ‰€æœ‰å‚æ•°ä¸å†»ç»“ï¼‰")
    print("=" * 70)
    
    # è§£é™¤æ‰€æœ‰æ¨¡å—çš„å†»ç»“
    print("\nè§£é™¤æ‰€æœ‰æ¨¡å—çš„å†»ç»“çŠ¶æ€...")
    
    # è§£é™¤æœå‘æ¨¡å‹çš„å†»ç»“
    orientation_model = cast(nn.Module, trainer.model.orientation_model)
    for param in orientation_model.parameters():
        param.requires_grad = True
    print("âœ“ Stage 1 (æœå‘åˆ†ç±»å™¨): å·²è§£é™¤å†»ç»“ï¼Œå‚æ•°å¯è®­ç»ƒ")
    
    # è§£é™¤ç»„ä»¶æ£€æµ‹æ¨¡å‹çš„å†»ç»“
    for ori_name, component_model in trainer.model.component_models.items():
        if component_model is not None:
            for param in component_model.parameters():
                param.requires_grad = True
    print("âœ“ Stage 2 (ç»„ä»¶æ£€æµ‹å™¨): å·²è§£é™¤å†»ç»“ï¼Œå‚æ•°å¯è®­ç»ƒ")
    
    # åˆ†ç±»å¤´å§‹ç»ˆå¯è®­ç»ƒ
    for param in trainer.model.flood_classifier.parameters():
        param.requires_grad = True
    print("âœ“ Stage 3 (æ´ªæ°´ç­‰çº§è¯„ä¼°å¤´): å‚æ•°å¯è®­ç»ƒ")
    
    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåŒ…å«æ‰€æœ‰å‚æ•°ï¼‰
    trainable_params = [p for p in trainer.model.parameters() if p.requires_grad]
    trainer.optimizer = torch.optim.AdamW(
        trainable_params,
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
    total_params = sum(p.numel() for p in trainer.model.parameters())
    trainable_params_count = sum(p.numel() for p in trainable_params)
    
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params_count:,} ({trainable_params_count/total_params*100:.2f}%)")
    print(f"  å†»ç»“å‚æ•°: 0 (0.00%)")
    
    print("\n" + "=" * 70)
    print("âœ… ä½¿ç”¨ç®€å•æ‹¼æ¥èåˆæ¨¡å‹ï¼ˆæ¶ˆèå®éªŒæ¶æ„ï¼‰")
    print("ğŸ“‹ é…ç½®: å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰| Weighted CE | ç±»åˆ«æƒé‡ | æ— è¿‡é‡‡æ · | æ— æ•°æ®å¢å¼º")
    print(f"ğŸ“Š æ•°æ®é›†: è®­ç»ƒ{len(trainer.train_dataset)}å¼  | éªŒè¯{len(trainer.val_dataset)}å¼ ")
    print("=" * 70)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 70)
    print("å¼€å§‹è®­ç»ƒï¼ˆæ‰€æœ‰å‚æ•°å¯è®­ç»ƒï¼‰")
    print("=" * 70)
    trainer.train()
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ2å®Œæˆ")
    print("=" * 70)

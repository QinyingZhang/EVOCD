"""
è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ1ï¼šåˆ†é˜¶æ®µå†»ç»“ç­–ç•¥ï¼ˆPhased Freezingï¼‰
- ç­–ç•¥ï¼šStage 1 å’Œ Stage 2 å†»ç»“ï¼Œåªè®­ç»ƒ Stage 3
- æŸå¤±å‡½æ•°ï¼šç­–ç•¥ä¸‰ï¼ˆFocal Loss + ç±»åˆ«æƒé‡ï¼Œæ— è¿‡é‡‡æ ·ï¼Œæ— æ•°æ®å¢å¼ºï¼‰
- æ¨¡å‹æ¶æ„ï¼šç®€å•æ‹¼æ¥èåˆ
"""
import argparse
import sys
from pathlib import Path
import torch

SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent.parent
HIER_MULT_DIR = PROJECT_ROOT / 'hier_mult'
if str(HIER_MULT_DIR) not in sys.path:
    sys.path.insert(0, str(HIER_MULT_DIR))

from archived.model_concat_fusion import HierarchicalFloodClassifier_ConcatFusion  # type: ignore
from settings import ORIENTATION_MODEL, COMPONENT_MODELS, NUM_FLOOD_GRADES  # type: ignore
from train import Trainer  # type: ignore


if __name__ == '__main__':
    print("=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ1ï¼šåˆ†é˜¶æ®µå†»ç»“ç­–ç•¥")
    print("ç­–ç•¥: Stage 1 å’Œ Stage 2 å†»ç»“ï¼Œåªè®­ç»ƒ Stage 3")
    print("=" * 70)
    
    # è·å–æ•°æ®é›†ç›®å½•
    DATA_DIR = PROJECT_ROOT / 'All_ablation_experiments' / 'data'
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='è®­ç»ƒç­–ç•¥æ¶ˆèï¼šåˆ†é˜¶æ®µå†»ç»“')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_csv', type=str, default=str(DATA_DIR / 'train.csv'))
    parser.add_argument('--val_csv', type=str, default=str(DATA_DIR / 'val.csv'))
    parser.add_argument('--test_csv', type=str, default=str(DATA_DIR / 'test.csv'))
    parser.add_argument('--image_size', type=int, default=640)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--pin_memory', type=bool, default=True)
    
    # æ¨¡å‹å‚æ•° - å…³é”®ï¼šå†»ç»“ Stage 1 å’Œ Stage 2
    parser.add_argument('--freeze_backbone', type=bool, default=True)  # å†»ç»“éª¨å¹²ç½‘ç»œ
    parser.add_argument('--use_component_branch', type=bool, default=True)
    
    # æ•°æ®å¢å¼º - ç­–ç•¥ä¸‰ï¼šæ— æ•°æ®å¢å¼ºï¼Œæ— è¿‡é‡‡æ ·
    parser.add_argument('--augment', type=bool, default=False)
    parser.add_argument('--oversample', type=bool, default=False)
    parser.add_argument('--oversample_level0', type=int, default=1)
    parser.add_argument('--oversample_level1', type=int, default=1)
    
    # æŸå¤±å‡½æ•° - ç­–ç•¥ä¸‰ï¼šFocal Loss + ç±»åˆ«æƒé‡
    parser.add_argument('--use_class_weights', type=bool, default=True)
    parser.add_argument('--label_smoothing', type=float, default=0.0)
    parser.add_argument('--use_focal_loss', type=bool, default=True)
    parser.add_argument('--focal_gamma', type=float, default=1.5)
    
    # è®­ç»ƒç­–ç•¥
    parser.add_argument('--mixed_precision', type=bool, default=True)
    parser.add_argument('--patience', type=int, default=20)
    parser.add_argument('--min_delta', type=float, default=1e-4)
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument('--print_freq', type=int, default=10)
    parser.add_argument('--save_interval', type=int, default=10)
    parser.add_argument('--resume', type=str, default=None)
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--runs_dir', type=str, default=str(SCRIPT_DIR / 'runs' / 'phased_freezing'))
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(args)
    
    # æ›¿æ¢æ¨¡å‹ä¸ºç®€å•æ‹¼æ¥èåˆç‰ˆæœ¬
    print("\næ›¿æ¢ä¸ºæ¶ˆèå®éªŒæ¨¡å‹ï¼ˆç®€å•æ‹¼æ¥èåˆï¼‰...")
    component_paths = {k: str(v) for k, v in COMPONENT_MODELS.items()}
    
    trainer.model = HierarchicalFloodClassifier_ConcatFusion(
        orientation_model_path=str(ORIENTATION_MODEL),
        component_model_paths=component_paths,
        num_flood_classes=NUM_FLOOD_GRADES,
        freeze_backbone=True,  # å†»ç»“ï¼Œä½¿ç”¨åˆ†é˜¶æ®µå†»ç»“ç­–ç•¥
        use_component_branch=True
    ).to(trainer.device)
    
    print("âœ… æ¨¡å‹æ›¿æ¢å®Œæˆ")
    print(f"èåˆæ–¹å¼: ç®€å•æ‹¼æ¥ï¼ˆConcatï¼‰")
    
    # åˆ†é˜¶æ®µå†»ç»“ç­–ç•¥ï¼šStage 1 å’Œ Stage 2 ä¿æŒå†»ç»“ï¼Œåªè®­ç»ƒ Stage 3
    print("\n" + "=" * 70)
    print("åˆ†é˜¶æ®µå†»ç»“è®­ç»ƒç­–ç•¥")
    print("=" * 70)
    print("\nå†»ç»“çŠ¶æ€æ£€æŸ¥...")
    
    # ç¡®ä¿ Stage 1 å’Œ Stage 2 å†»ç»“
    for param in trainer.model.orientation_model.parameters():  # type: ignore
        param.requires_grad = False
    print("âœ“ Stage 1 (æœå‘åˆ†ç±»å™¨): å†»ç»“ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ")
    
    for ori_name, component_model in trainer.model.component_models.items():
        if component_model is not None:
            for param in component_model.parameters():
                param.requires_grad = False
    print("âœ“ Stage 2 (ç»„ä»¶æ£€æµ‹å™¨): å†»ç»“ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ")
    
    # Stage 3 å¯è®­ç»ƒ
    for param in trainer.model.flood_classifier.parameters():
        param.requires_grad = True
    print("âœ“ Stage 3 (æ´ªæ°´ç­‰çº§è¯„ä¼°å¤´): å¯è®­ç»ƒ")
    
    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåªåŒ…å«å¯è®­ç»ƒå‚æ•°ï¼‰
    trainable_params = [p for p in trainer.model.parameters() if p.requires_grad]
    trainer.optimizer = torch.optim.AdamW(
        trainable_params,
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in trainer.model.parameters())
    trainable_params_count = sum(p.numel() for p in trainable_params)
    frozen_params_count = total_params - trainable_params_count
    
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params_count:,} ({trainable_params_count/total_params*100:.2f}%)")
    print(f"  å†»ç»“å‚æ•°: {frozen_params_count:,} ({frozen_params_count/total_params*100:.2f}%)")
    
    print("\n" + "=" * 70)
    print("âœ… ä½¿ç”¨ç®€å•æ‹¼æ¥èåˆæ¨¡å‹ï¼ˆæ¶ˆèå®éªŒæ¶æ„ï¼‰")
    print("ğŸ“‹ é…ç½®: åˆ†é˜¶æ®µå†»ç»“ç­–ç•¥ | Focal Loss (Î³=1.5) | ç±»åˆ«æƒé‡ | æ— æ•°æ®å¢å¼º | æ— è¿‡é‡‡æ ·")
    print(f"ğŸ“Š æ•°æ®é›†: è®­ç»ƒ{len(trainer.train_dataset)}å¼  | éªŒè¯{len(trainer.val_dataset)}å¼ ")
    print("=" * 70)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 70)
    print("å¼€å§‹è®­ç»ƒï¼ˆåªè®­ç»ƒ Stage 3ï¼‰")
    print("=" * 70)
    trainer.train()
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ1ï¼ˆåˆ†é˜¶æ®µå†»ç»“ï¼‰å®Œæˆ")
    print("=" * 70)

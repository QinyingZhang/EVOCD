"""
è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ3ï¼šå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆä¸å†»ç»“ï¼‰
- ç­–ç•¥ï¼šåˆ†é˜¶æ®µè®­ç»ƒä½†ä¸å†»ç»“ï¼Œæ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ
- æŸå¤±å‡½æ•°ï¼šç­–ç•¥ä¸‰ï¼ˆFocal Loss + ç±»åˆ«æƒé‡ï¼Œæ— è¿‡é‡‡æ ·ï¼Œæ— æ•°æ®å¢å¼ºï¼‰
- æ¨¡å‹æ¶æ„ï¼šç®€å•æ‹¼æ¥èåˆ
"""
import argparse
import sys
from pathlib import Path
import torch

SCRIPT_DIR = Path(__file__).resolve().parent
# 0: training_strategies, 1: All_ablation_experiments, 2: ultralytics-main
PROJECT_ROOT = SCRIPT_DIR.parents[2]
HIER_MULT_DIR = PROJECT_ROOT / 'hier_mult'
if str(HIER_MULT_DIR) not in sys.path:
    sys.path.insert(0, str(HIER_MULT_DIR))
# ç¡®ä¿åŒ…å¯¼å…¥å¯ç”¨ï¼ˆhier_mult.*ï¼‰
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
# ç¡®ä¿èƒ½å¯¼å…¥ hier_mult/archived ä¸‹çš„æ¨¡å‹
HIER_MULT_ARCHIVED = HIER_MULT_DIR / 'archived'
if str(HIER_MULT_ARCHIVED) not in sys.path:
    sys.path.insert(0, str(HIER_MULT_ARCHIVED))

# ä½¿ç”¨åŒ…è·¯å¾„å¯¼å…¥ï¼Œé¿å…ç›¸å¯¹è·¯å¾„é—®é¢˜
import importlib.util

# åŠ¨æ€åŠ è½½ archived/model_concat_fusion.py
spec_model = importlib.util.spec_from_file_location(
    "model_concat_fusion", str(HIER_MULT_DIR / "archived" / "model_concat_fusion.py")
)
assert spec_model is not None and spec_model.loader is not None
model_module = importlib.util.module_from_spec(spec_model)
spec_model.loader.exec_module(model_module)
HierarchicalFloodClassifier_ConcatFusion = model_module.HierarchicalFloodClassifier_ConcatFusion

# ç›´æ¥ä» hier_mult ç›®å½•åŠ è½½ settings.py
spec_settings = importlib.util.spec_from_file_location(
    "settings", str(HIER_MULT_DIR / "settings.py")
)
assert spec_settings is not None and spec_settings.loader is not None
settings = importlib.util.module_from_spec(spec_settings)
spec_settings.loader.exec_module(settings)
ORIENTATION_MODEL = settings.ORIENTATION_MODEL
COMPONENT_MODELS = settings.COMPONENT_MODELS
NUM_FLOOD_GRADES = settings.NUM_FLOOD_GRADES

# Trainer ä» hier_mult/train.py
spec_train = importlib.util.spec_from_file_location(
    "train_module", str(HIER_MULT_DIR / "train.py")
)
assert spec_train is not None and spec_train.loader is not None
train_module = importlib.util.module_from_spec(spec_train)
spec_train.loader.exec_module(train_module)
Trainer = train_module.Trainer


if __name__ == '__main__':
    print("=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ3ï¼šå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆä¸å†»ç»“ï¼‰")
    print("ç­–ç•¥: åˆ†é˜¶æ®µè®­ç»ƒï¼Œä½†æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰")
    print("=" * 70)
    
    # è·å–æ•°æ®é›†ç›®å½•
    DATA_DIR = PROJECT_ROOT / 'All_ablation_experiments' / 'data'
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='è®­ç»ƒç­–ç•¥æ¶ˆèï¼šå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_csv', type=str, default=str(DATA_DIR / 'train.csv'))
    parser.add_argument('--val_csv', type=str, default=str(DATA_DIR / 'val.csv'))
    parser.add_argument('--test_csv', type=str, default=str(DATA_DIR / 'test.csv'))
    parser.add_argument('--image_size', type=int, default=640)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--pin_memory', type=bool, default=True)
    
    # æ¨¡å‹å‚æ•° - å…³é”®ï¼šä¸å†»ç»“ä»»ä½•æ¨¡å—
    parser.add_argument('--freeze_backbone', type=bool, default=False)  # ä¸å†»ç»“éª¨å¹²ç½‘ç»œ
    parser.add_argument('--use_component_branch', type=bool, default=True)
    
    # æ•°æ®å¢å¼º - ç­–ç•¥ä¸‰ï¼šæ— æ•°æ®å¢å¼ºï¼Œæ— è¿‡é‡‡æ ·
    parser.add_argument('--augment', type=bool, default=False)
    parser.add_argument('--oversample', type=bool, default=False)
    parser.add_argument('--oversample_level0', type=int, default=1)
    parser.add_argument('--oversample_level1', type=int, default=1)
    
    # æŸå¤±å‡½æ•° - ç­–ç•¥ä¸‰ï¼šFocal Loss + ç±»åˆ«æƒé‡
    parser.add_argument('--use_class_weights', type=bool, default=True)
    parser.add_argument('--label_smoothing', type=float, default=0.0)
    parser.add_argument('--use_focal_loss', type=bool, default=True)
    parser.add_argument('--focal_gamma', type=float, default=1.5)
    
    # è®­ç»ƒç­–ç•¥
    parser.add_argument('--mixed_precision', type=bool, default=True)
    parser.add_argument('--patience', type=int, default=20)
    parser.add_argument('--min_delta', type=float, default=1e-4)
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument('--print_freq', type=int, default=10)
    parser.add_argument('--save_interval', type=int, default=10)
    parser.add_argument('--resume', type=str, default=None)
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--runs_dir', type=str, default=str(SCRIPT_DIR / 'runs' / 'progressive_training'))
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(args)
    
    # æ›¿æ¢æ¨¡å‹ä¸ºç®€å•æ‹¼æ¥èåˆç‰ˆæœ¬
    print("\næ›¿æ¢ä¸ºæ¶ˆèå®éªŒæ¨¡å‹ï¼ˆç®€å•æ‹¼æ¥èåˆï¼‰...")
    component_paths = {k: str(v) for k, v in COMPONENT_MODELS.items()}
    
    trainer.model = HierarchicalFloodClassifier_ConcatFusion(
        orientation_model_path=str(ORIENTATION_MODEL),
        component_model_paths=component_paths,
        num_flood_classes=NUM_FLOOD_GRADES,
        freeze_backbone=False,  # ä¸å†»ç»“ï¼Œä½¿ç”¨å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒ
        use_component_branch=True
    ).to(trainer.device)
    
    print("âœ… æ¨¡å‹æ›¿æ¢å®Œæˆ")
    print(f"èåˆæ–¹å¼: ç®€å•æ‹¼æ¥ï¼ˆConcatï¼‰")
    
    # å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šè§£é™¤æ‰€æœ‰æ¨¡å—çš„å†»ç»“
    print("\n" + "=" * 70)
    print("å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ˆæ‰€æœ‰å‚æ•°ä¸å†»ç»“ï¼‰")
    print("=" * 70)
    print("\nè§£é™¤æ‰€æœ‰æ¨¡å—çš„å†»ç»“çŠ¶æ€...")
    
    # è§£é™¤æœå‘æ¨¡å‹çš„å†»ç»“
    for param in trainer.model.orientation_model.parameters():  # type: ignore
        param.requires_grad = True
    print("âœ“ Stage 1 (æœå‘åˆ†ç±»å™¨): å·²è§£é™¤å†»ç»“ï¼Œå‚æ•°å¯è®­ç»ƒ")
    
    # è§£é™¤ç»„ä»¶æ£€æµ‹æ¨¡å‹çš„å†»ç»“
    for ori_name, component_model in trainer.model.component_models.items():
        if component_model is not None:
            for param in component_model.parameters():
                param.requires_grad = True
    print("âœ“ Stage 2 (ç»„ä»¶æ£€æµ‹å™¨): å·²è§£é™¤å†»ç»“ï¼Œå‚æ•°å¯è®­ç»ƒ")
    
    # åˆ†ç±»å¤´å§‹ç»ˆå¯è®­ç»ƒ
    for param in trainer.model.flood_classifier.parameters():
        param.requires_grad = True
    print("âœ“ Stage 3 (æ´ªæ°´ç­‰çº§è¯„ä¼°å¤´): å‚æ•°å¯è®­ç»ƒ")
    
    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåŒ…å«æ‰€æœ‰å‚æ•°ï¼‰
    trainable_params = [p for p in trainer.model.parameters() if p.requires_grad]
    trainer.optimizer = torch.optim.AdamW(
        trainable_params,
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in trainer.model.parameters())
    trainable_params_count = sum(p.numel() for p in trainable_params)
    
    print(f"\nå‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params_count:,} ({trainable_params_count/total_params*100:.2f}%)")
    print(f"  å†»ç»“å‚æ•°: 0 (0.00%)")
    
    print("\n" + "=" * 70)
    print("åˆ†é˜¶æ®µè®­ç»ƒè¯´æ˜:")
    print("=" * 70)
    print("ç­–ç•¥: åˆ†é˜¶æ®µè®­ç»ƒä½†ä¸å†»ç»“")
    print("â€¢ Stage 1 å’Œ Stage 2 ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–")
    print("â€¢ è®­ç»ƒ Stage 3 æ—¶ï¼Œæ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰")
    print("â€¢ è¿™å…è®¸æ¨¡å‹åœ¨ç«¯åˆ°ç«¯å­¦ä¹ ä¸­è‡ªé€‚åº”è°ƒæ•´æ‰€æœ‰æ¨¡å—")
    
    print("\n" + "=" * 70)
    print("âœ… ä½¿ç”¨ç®€å•æ‹¼æ¥èåˆæ¨¡å‹ï¼ˆæ¶ˆèå®éªŒæ¶æ„ï¼‰")
    print("ğŸ“‹ é…ç½®: å®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒï¼ˆä¸å†»ç»“ï¼‰ | Focal Loss (Î³=1.5) | ç±»åˆ«æƒé‡ | æ— æ•°æ®å¢å¼º | æ— è¿‡é‡‡æ ·")
    print(f"ğŸ“Š æ•°æ®é›†: è®­ç»ƒ{len(trainer.train_dataset)}å¼  | éªŒè¯{len(trainer.val_dataset)}å¼ ")
    print("=" * 70)
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 70)
    print("å¼€å§‹è®­ç»ƒï¼ˆæ‰€æœ‰å‚æ•°å¯è®­ç»ƒï¼‰")
    print("=" * 70)
    trainer.train()
    
    print("\n" + "=" * 70)
    print("è®­ç»ƒç­–ç•¥æ¶ˆèå®éªŒ3ï¼ˆå®Œå…¨åˆ†é˜¶æ®µè®­ç»ƒï¼‰å®Œæˆ")
    print("=" * 70)
